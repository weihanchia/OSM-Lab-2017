{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 7\n",
    "\n",
    "#### 9.3\n",
    "Steepest Descent:\n",
    "\n",
    "(i) Using the gradient of the function, we move in each iteration in the direction of the gradient, which is a descent direction. Geometrically, we are following the slope of the function to the valley (a minimum point)\n",
    "\n",
    "(ii) It can solve unconstrained optimization problems with a differentiable function to find a local minimum.\n",
    "\n",
    "(iii) Computationally inexpensive as we only need to evaluate the gradient at each point\n",
    "\n",
    "(iv) Slow convergence compared to the other methods, as well as being dependent on the starting point.\n",
    "\n",
    "Newton's Method:\n",
    "\n",
    "(i) We use a quadratic approximation to the function at that point, and solve for the direction to find the minimum of this quadratic approximation. This involves solving for the inverse of the Hessian at the point, and moving in the direction of $-Df(x) / D^2f(x)$. \n",
    "\n",
    "(ii) It can solve unconstrainted optimization problems with a twice differentiable function to find a local minimum\n",
    "\n",
    "(iii) Quick convergence (quadratic).\n",
    "\n",
    "(iv) Highly dependent on starting point, and computationally expensive as we have to compute a second derivative (Hessian)\n",
    "\n",
    "Quasi-Newton Methods (Broyden, BFGS):\n",
    "\n",
    "(i) We use a quadratic approximation to the function at a point, and solve for the direction to find the minimum of this quadratic approximation. However rather than solving for the inverse of the Hessian, we use approximations to obtain an approximate value for the Hessian.\n",
    "\n",
    "(ii) It solves unconstrained optimization problems with a differentiable function to find a local minimum.\n",
    "\n",
    "(iii) Less Computationally expensive compared to Newton's Method, yet with faster convergence than steepest descent.\n",
    "\n",
    "(iv) Still more computationally expensive than steepest descent, dependent on starting point.\n",
    "\n",
    "Conjugate Gradient:\n",
    "\n",
    "(i) Rather than moving back and forth according to the direction of the gradient, conjugate gradient methods move in a conjugate direction\n",
    "\n",
    "(ii) It can solve unconstrained optimizatoin problems with a differentiable function to find a local minimum.\n",
    "\n",
    "(iii) Less computationally expensive than Newton/Quasi-Newton methods, faster convergence than steepest descent\n",
    "\n",
    "(iv) More comptutationally expensive than Steepest Descent, slower convergence than Newton/Quasi-Newton methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sdquad(Q, b, xguess, eps = 1e-2, maxit = 1000):\n",
    "    it = 1\n",
    "    error = 1\n",
    "    x0 = xguess\n",
    "    while error > eps and it < maxit:\n",
    "        Df = np.dot(Q, x0) - b\n",
    "        if Df == 0:\n",
    "            return x0\n",
    "        else:\n",
    "            alphak = (np.dot(Df, Df))/(np.dot(Df, np.dot(Q, Df)))\n",
    "            x1 = x0 - alphak*Df\n",
    "            error = abs(Df)\n",
    "            x0 = x1\n",
    "    return x0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fdiff(f, x, eps=1e-2):\n",
    "    f1 = f(x + eps)\n",
    "    f0 = f(x)\n",
    "    return (f1 - f0)/eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the quadratic function $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^T Q \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$. Now we know that the unique minimizer of $f$ is $Q^{-1} \\mathbf{b}$. \n",
    "\n",
    "Let us start at any $\\mathbf{x}_0$. Then we have that our Newton direction will be $Q^{-1}(Q \\mathbf{x}_0 - \\mathbf{b})$\n",
    "\n",
    "Now $\\mathbf{x}_1 = \\mathbf{x}_0 - Q^{-1}(Q \\mathbf{x}_0 - b) = Q^{-1} \\mathbf{b}$\n",
    "\n",
    "And so in 1 iteration we have arrived at the unique minimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
